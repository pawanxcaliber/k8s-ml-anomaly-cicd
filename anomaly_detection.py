# anomaly_detection.py - Updated for Multi-Modal Data Loading, Merging, FE, and Anomaly Detection

import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import MinMaxScaler, StandardScaler # Add StandardScaler for multi-modal FE
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import os

# --- Configuration ---
# Input files generated by script.py
NODE_METRICS_FILE = 'node_metrics_raw.csv'
PER_POD_METRICS_FILE = 'per_pod_metrics_raw.csv' # This now contains per-pod CPU/Memory
# Placeholder files for logs and network - will be added later
# LOGS_FILE = 'container_logs_raw.csv'
# NETWORK_FILE = 'network_flows_raw.csv'

# File to save/load the trained Autoencoder model
AUTOENCODER_MODEL_FILE = 'autoencoder_model.keras' # Keep the same model file name

# --- Data Loading ---
print(f"Loading node metrics from '{NODE_METRICS_FILE}'...")
try:
    # Load node metrics, timestamp is the index
    node_data = pd.read_csv(NODE_METRICS_FILE, index_col=0, parse_dates=True)
    print(f"Node metrics loaded successfully. Shape: {node_data.shape}")
    print(node_data.head())
except FileNotFoundError:
    print(f"Error: '{NODE_METRICS_FILE}' not found. Please run script.py first.")
    exit()
except Exception as e:
    print(f"Error loading node metrics from '{NODE_METRICS_FILE}': {e}")
    exit()

print(f"\nLoading per-pod metrics from '{PER_POD_METRICS_FILE}'...")
try:
    # Load per-pod metrics, timestamp is a column
    per_pod_data = pd.read_csv(PER_POD_METRICS_FILE, parse_dates=['timestamp'])
    print(f"Per-pod metrics loaded successfully. Shape: {per_pod_data.shape}")
    print(per_pod_data.head())
except FileNotFoundError:
    print(f"Error: '{PER_POD_METRICS_FILE}' not found. Please run script.py first.")
    exit()
except Exception as e:
    print(f"Error loading per-pod metrics from '{PER_POD_METRICS_FILE}': {e}")
    exit()

# --- Data Alignment and Merging (Per-Pod -> Node Level) ---
print("\nAggregating per-pod metrics and merging with node metrics...")

# Identify metric columns in the per-pod data (those that are not labels or timestamp)
# Assuming labels are non-numeric strings, select numeric columns
per_pod_metric_cols = per_pod_data.select_dtypes(include=np.number).columns.tolist()
if 'timestamp' in per_pod_metric_cols:
     per_pod_metric_cols.remove('timestamp') # Ensure timestamp isn't included in numeric sum

if not per_pod_metric_cols:
     print("Warning: No numeric metric columns found in per-pod data after loading. Per-pod data cannot be aggregated.")
     # Create an empty aggregated dataframe to continue
     # Use the index from node data for alignment
     aggregated_per_pod_data = pd.DataFrame(index=node_data.index)
else:
     print(f"Aggregating per-pod metric columns: {per_pod_metric_cols}")
     # Aggregate per-pod metrics to the node level by summing values for each timestamp
     # Set timestamp as index temporarily for aggregation
     aggregated_per_pod_data = per_pod_data.set_index('timestamp').groupby(level=0)[per_pod_metric_cols].sum()
     # Rename columns to reflect aggregation
     aggregated_per_pod_data.columns = [f'aggregated_{col}' for col in aggregated_per_pod_data.columns]
     print(f"Aggregated per-pod data shape: {aggregated_per_pod_data.shape}")
     print(aggregated_per_pod_data.head())

# Merge the aggregated per-pod data with the node metrics on the timestamp index
# Use outer join to keep all timestamps from both datasets
combined_data = node_data.join(aggregated_per_pod_data, how='outer')

# Ensure a consistent time index for the combined data
# Use the frequency from the original node data index if available, or infer it.
# Assuming node_data index has a frequency
if node_data.index.freq is not None:
    combined_data = combined_data.asfreq(node_data.index.freq)
elif not combined_data.empty:
    # Infer frequency from the combined data index after join
    try:
        inferred_freq = pd.infer_freq(combined_data.index)
        if inferred_freq:
            combined_data = combined_data.asfreq(inferred_freq)
        else:
             print("Warning: Could not infer frequency from combined data index. Skipping asfreq.")
    except Exception as e:
         print(f"Warning: Error inferring frequency: {e}. Skipping asfreq.")

# Interpolate and fill NaNs introduced by the outer join and asfreq
combined_data = combined_data.interpolate(method='linear')
# Use the newer method calls for fillna to avoid FutureWarning
combined_data.ffill(inplace=True) # Forward fill from earlier values
combined_data.bfill(inplace=True) # Back fill from later values
combined_data.fillna(0, inplace=True) # Fill any remaining NaNs with 0

print(f"\n--- Combined Data (Node + Aggregated Per-Pod) ---")
print(f"{len(combined_data)} samples, {len(combined_data.columns)} features (before FE)")
print(combined_data.head())
print("...")
print(combined_data.tail())

if combined_data.empty:
    print("Error: Combined data is empty after merging and cleaning. Cannot proceed with feature engineering or anomaly detection.")
    exit()


# --- Feature Engineering (Apply to Combined Data) ---
print("\n--- Performing Feature Engineering on Combined Data ---")

# Identify columns to apply time-series features to.
# Use all numeric columns in the combined data.
time_series_cols_to_process = combined_data.select_dtypes(include=np.number).columns.tolist()

if not time_series_cols_to_process:
     print("Error: No numeric columns found in combined data for feature engineering.")
     exit()

# Rate of Change
for col in time_series_cols_to_process:
     combined_data[f'{col}_diff'] = combined_data[col].diff()
print("Calculated Rate of Change features.")

# Rolling Statistics (Use a window size based on step duration, like 5 minutes)
# Assuming the index frequency is consistent and represents the step duration
if combined_data.index.freq is not None:
    window_duration = pd.Timedelta('5m') # Example: 5-minute rolling window
    # Calculate window size in steps based on the data's frequency
    # Handle cases where frequency might be unusual, approximate if needed
    try:
         # Ensure frequency calculation is robust
         freq_seconds = combined_data.index.freq.total_seconds()
         if freq_seconds > 0:
             rolling_window_steps = int(window_duration.total_seconds() / freq_seconds)
         else:
             rolling_window_steps = 20 # Fallback if frequency is zero or calculation fails

         if rolling_window_steps < 1: rolling_window_steps = 1 # Ensure window is at least 1 step
         if rolling_window_steps > len(combined_data): rolling_window_steps = len(combined_data) # Don't exceed data length
         print(f"Calculating rolling stats over a {rolling_window_steps}-step window ({window_duration}) based on index frequency.")
    except (TypeError, AttributeError, ValueError): # Handle cases where freq is None or calculation fails
         # Fallback to a fixed number of steps if frequency is not inferrable or usable
         rolling_window_steps = 20 # Example: fixed 20 steps
         if rolling_window_steps > len(combined_data): rolling_window_steps = len(combined_data)
         print(f"Warning: Could not use index frequency for rolling window. Calculating rolling stats over a fixed {rolling_window_steps}-step window.")

    for col in time_series_cols_to_process:
         # Use .rolling() with min_periods=1 to get stats even at the very beginning
         combined_data[f'{col}_rolling_mean'] = combined_data[col].rolling(window=rolling_window_steps, min_periods=1).mean()
         combined_data[f'{col}_rolling_std'] = combined_data[col].rolling(window=rolling_window_steps, min_periods=1).std()
    print("Calculated Rolling Statistics features.")

else:
    print("Warning: Cannot perform rolling statistics feature engineering: Combined data index frequency is not set.")
    # Or implement rolling stats using a fixed number of steps here as a fallback


# Lagged Values (e.g., 1 previous step)
lag_size = 1 # Lag by 1 time step
print(f"Calculating {lag_size}-step lagged features")
for col in time_series_cols_to_process:
     combined_data[f'{col}_lag{lag_size}'] = combined_data[col].shift(lag_size)
print("Calculated Lagged features.")

# Interaction Features (Ratios) - Apply to original metrics before differencing/rolling if possible
# Add a small epsilon to the denominator to avoid division by zero or near-zero issues
epsilon = 1e-9
# Example Ratio: Aggregated Per-Pod CPU / Node CPU
if 'aggregated_pod_cpu_usage' in combined_data.columns and 'cpu_usage' in combined_data.columns:
    combined_data['agg_pod_cpu_to_node_cpu_ratio'] = combined_data['aggregated_pod_cpu_usage'] / (combined_data['cpu_usage'] + epsilon)
    combined_data['agg_pod_cpu_to_node_cpu_ratio'] = combined_data['agg_pod_cpu_to_node_cpu_ratio'].replace([np.inf, -np.inf], np.nan) # Replace infinities with NaN
    print("Calculated 'agg_pod_cpu_to_node_cpu_ratio' feature.")

# Example Ratio: Aggregated Per-Pod Memory / Node Memory
if 'aggregated_pod_memory_usage' in combined_data.columns and 'memory_usage' in combined_data.columns:
    combined_data['agg_pod_mem_to_node_mem_ratio'] = combined_data['aggregated_pod_memory_usage'] / (combined_data['memory_usage'] + epsilon)
    combined_data['agg_pod_mem_to_node_mem_ratio'] = combined_data['agg_pod_mem_to_node_mem_ratio'].replace([np.inf, -np.inf], np.nan)
    print("Calculated 'agg_pod_mem_to_node_mem_ratio' feature.")

# Node Network Traffic Ratio (Receive vs Transmit) - Already in original FE, ensuring it's applied here
if 'network_receive_bytes' in combined_data.columns and 'network_transmit_bytes' in combined_data.columns:
    combined_data['node_net_rx_tx_ratio'] = combined_data['network_receive_bytes'] / (combined_data['network_transmit_bytes'] + epsilon)
    combined_data['node_net_rx_tx_ratio'] = combined_data['node_net_rx_tx_ratio'].replace([np.inf, -np.inf], np.nan)
    print("Calculated 'node_net_rx_tx_ratio' feature.")


# Time Features (Hour of the day)
combined_data['hour_of_day'] = combined_data.index.hour
print("Added 'hour_of_day' feature.")

print(f"\n--- Combined Data (after Feature Engineering) ---")
print(f"{len(combined_data)} samples, {len(combined_data.columns)} features")
print(combined_data.head())
print("...")
print(combined_data.tail())

# --- Handle NaNs Introduced by Feature Engineering ---
print("\nHandling NaNs introduced by feature engineering...")

# Use interpolation first
combined_data = combined_data.interpolate(method='linear')

# Then use ffill and bfill to handle NaNs at the boundaries that interpolation missed
combined_data.ffill(inplace=True) # Forward fill
combined_data.bfill(inplace=True) # Back fill

# Finally, fill any remaining NaNs with 0 (e.g., if an entire new feature column was NaN or Inf)
combined_data.fillna(0, inplace=True)

print("NaNs handled.")
print(f"Final Data shape after FE and NaN handling: {combined_data.shape}")

# --- Scaling Data ---
print("\n--- Scaling Data ---")
# Apply scaler to the *entire* dataframe now containing original and new features
scaler = StandardScaler() # Using StandardScaler as done previously
# Use .values to get the numpy array for scaling
data_normalized = scaler.fit_transform(combined_data.values)

print("Scaling complete.")
print(f"Normalized Data shape: {data_normalized.shape}")

# Prepare the feature names after FE for potential use in analysis/visualization
feature_names = combined_data.columns.tolist()
input_dim = data_normalized.shape[1] # Update input dimension based on final feature count

print("\n--- Processed & Normalized Data (Numpy Array Preview) ---")
# Print only the first few rows of the array for brevity
if len(data_normalized) > 5:
    print(data_normalized[:5])
    print("...")
    # print the last few rows if the array is long enough
    print(data_normalized[-5:])
else:
     print(data_normalized)

if data_normalized.size == 0:
    print("Error: Normalized data is empty. Cannot proceed with anomaly detection models.")
    exit()


# --- Anomaly Detection Models ---
# Apply the models from the previous anomaly_detection.py to the multi-modal data
print("\nApplying Anomaly Detection Models to Multi-Modal Data...")

# 1. Isolation Forest
print("Training Isolation Forest...")
iforest = IsolationForest(n_estimators=200, contamination='auto', random_state=42, n_jobs=-1)
iforest.fit(data_normalized)
iforest_scores = -iforest.decision_function(data_normalized) # Invert scores
print("Isolation Forest trained and scores calculated.")


# 2. Autoencoder
print("\nSetting up and training Autoencoder...")
# Encoding dimension based on the new input_dim
encoding_dim = max(2, int(input_dim * 0.5))
print(f"Autoencoder encoding dimension: {encoding_dim}")

autoencoder = None
# Optional: Load model if it exists
if os.path.exists(AUTOENCODER_MODEL_FILE):
    try:
        # Custom objects might be needed if you used custom layers/activations
        autoencoder = load_model(AUTOENCODER_MODEL_FILE)
        # Check if the loaded model's input shape matches the current data shape
        if autoencoder.input_shape[1] == input_dim:
             print(f"Loaded Autoencoder model from '{AUTOENCODER_MODEL_FILE}' with matching input shape.")
        else:
             print(f"Loaded Autoencoder model has input shape {autoencoder.input_shape[1]}, but current data has {input_dim}. Re-training model.")
             autoencoder = None # Force re-training if shapes don't match
    except Exception as e:
        print(f"Error loading Autoencoder model: {e}. Re-training model.")
        autoencoder = None # Force re-training if loading fails


if autoencoder is None:
    # Define the Autoencoder model architecture
    input_layer = Input(shape=(input_dim,))
    encoder = Dense(encoding_dim * 2, activation='relu')(input_layer)
    encoder = Dense(encoding_dim, activation='relu')(encoder)
    decoder = Dense(encoding_dim * 2, activation='relu')(encoder)
    decoder = Dense(input_dim, activation='linear')(decoder) # Should decode back to original input dim

    autoencoder = Model(inputs=input_layer, outputs=decoder)

    # Compile the model
    optimizer = Adam(learning_rate=0.001)
    autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')

    # Train the model
    print("Training Autoencoder...")
    # Use validation_split=0.2 or 0.1 for monitoring, or 0 if training on all data
    history = autoencoder.fit(data_normalized, data_normalized,
                              epochs=100, # Number of training cycles
                              batch_size=64, # Number of samples per gradient update
                              shuffle=True, # Shuffle data for each epoch
                              verbose=0, # Suppress training output
                              validation_split=0.2) # Hold out data for validation
    print("Autoencoder training complete.")

    # Optional: Save the trained model
    try:
        # Use the 'keras' format recommended for TensorFlow 2.x
        autoencoder.save(AUTOENCODER_MODEL_FILE)
        print(f"Autoencoder model saved to '{AUTOENCODER_MODEL_FILE}'.")
    except Exception as e:
        print(f"Error saving Autoencoder model: {e}")

# Get reconstruction error (anomaly score for autoencoder)
y_pred_autoencoder = autoencoder.predict(data_normalized, verbose=0)
reconstruction_error = np.mean((data_normalized - y_pred_autoencoder) ** 2, axis=1)
print("Autoencoder reconstruction error calculated.")


# 3. One-Class SVM (OCSVM)
print("\nTraining One-Class SVM...")
ocsvm = OneClassSVM(nu=0.01, kernel="rbf", gamma="scale") # Adjust nu based on expected anomaly rate
ocsvm.fit(data_normalized)
# Get anomaly scores. The decision_function value's sign indicates normal/anomaly,
# and the magnitude is distance to the boundary.
ocsvm_scores = -ocsvm.decision_function(data_normalized) # Invert so higher is more anomalous
print("One-Class SVM trained and scores calculated.")


# --- Anomaly Score Combination and Thresholding ---
print("\nCombining anomaly scores from multiple models...")

# Normalize scores to a comparable range (0 to 1)
scaler_if = MinMaxScaler()
iforest_scores_scaled = scaler_if.fit_transform(iforest_scores.reshape(-1, 1)).flatten()

scaler_ae = MinMaxScaler()
ae_scores_scaled = scaler_ae.fit_transform(reconstruction_error.reshape(-1, 1)).flatten()

scaler_ocsvm = MinMaxScaler()
ocsvm_scores_scaled = scaler_ocsvm.fit_transform(ocsvm_scores.reshape(-1, 1)).flatten()

# Combine scores (Simple Average as a starting example)
# You could experiment with different methods: weighted average, max score, etc.
combined_anomaly_score = (iforest_scores_scaled + ae_scores_scaled + ocsvm_scores_scaled) / 3
print("Anomaly scores combined using simple average of scaled scores.")

# --- Thresholding ---
# Determine the threshold for the combined score.
# Using a percentile is a common way to set a threshold based on the score distribution.
# Adjust the percentile (e.g., 90, 95, 99) based on how many top anomalies you want to flag for investigation.
threshold_percentile = 95
threshold_value = np.percentile(combined_anomaly_score, threshold_percentile)
print(f"Threshold set at {threshold_percentile}th percentile of combined score: {threshold_value}")

# Classify data points as anomalous if their combined score exceeds the threshold
# Anomaly indices correspond to rows in the original combined_data and data_normalized
anomaly_indices = np.where(combined_anomaly_score > threshold_value)[0]
print(f'\nDetected Anomalies (Sample Indices): {anomaly_indices}')
print(f'Number of Detected Anomalies: {len(anomaly_indices)}')


# --- Visualization ---
plt.figure(figsize=(14, 7))
# Use the combined_data index (timestamps) for plotting
plt.plot(combined_data.index, combined_anomaly_score, label='Combined Anomaly Score', alpha=0.7)
plt.axhline(y=threshold_value, color='r', linestyle='--', label=f'Threshold ({threshold_percentile}th percentile)')

# Scatter plot the detected anomalies using the index from combined_data
if len(anomaly_indices) > 0:
    anomaly_timestamps = combined_data.index[anomaly_indices]
    plt.scatter(anomaly_timestamps, combined_anomaly_score[anomaly_indices],
                color='g', label='Anomalies', zorder=5) # Use zorder to ensure dots are on top

    # Optional: Plot one or two key metrics from the original combined_data on a secondary axis for context
    # (Requires careful handling of secondary axis and legend)
    # Example (uncomment and adapt if needed):
    # if 'cpu_usage' in combined_data.columns:
    #     ax2 = plt.gca().twinx()
    #     ax2.plot(combined_data.index, combined_data['cpu_usage'], color='orange', alpha=0.3, label='Node CPU Usage (Original)')
    #     ax2.set_ylabel('Node CPU Usage (Original)')
    #     ax2.tick_params(axis='y', labelcolor='orange')
    # if 'aggregated_pod_cpu_usage' in combined_data.columns:
    #      ax3 = plt.gca().twinx() # Potentially another twin axis or offset
    #      ax3.spines['right'].set_position(('outward', 60)) # Offset secondary axis
    #      ax3.plot(combined_data.index, combined_data['aggregated_pod_cpu_usage'], color='purple', alpha=0.3, label='Aggregated Pod CPU (Original)')
    #      ax3.set_ylabel('Aggregated Pod CPU (Original)')
    #      ax3.tick_params(axis='y', labelcolor='purple')
    # # Update legend to include secondary axes labels if added
    # lines, labels = plt.gca().get_legend_handles_labels()
    # if 'ax2' in locals(): lines += ax2.get_legend_handles_labels()[0]; labels += ax2.get_legend_handles_labels()[1]
    # if 'ax3' in locals(): lines += ax3.get_legend_handles_labels()[0]; labels += ax3.get_legend_handles_labels()[1]
    # plt.legend(lines, labels, loc='upper left')


else:
     print("No anomalies detected to scatter plot.")


plt.title('Anomaly Detection using Combined Multi-Modal Metric Scores') # Updated title
plt.xlabel('Timestamp')
plt.ylabel('Combined Anomaly Score (Scaled)')
plt.legend(loc='upper right')
plt.grid(True)
plt.tight_layout()
plt.show()


# --- Save Anomaly Indices ---
output_indices_file = 'anomaly_indices_combined_multi_modal.txt' # Updated output filename
np.savetxt(output_indices_file, anomaly_indices, fmt='%d')
print(f'\nAnomaly indices saved to "{output_indices_file}"')

# --- Further Steps (Relevant to Correlation & Fusion Phase) ---
# Now that you have detected anomalies based on combined node+aggregated per-pod metrics,
# the next steps for your research goal of detecting *tampering in pods* involve:
# 1. Investigating the flagged anomaly timestamps in your RAW data files (node_metrics_raw.csv, per_pod_metrics_raw.csv).
#    Look at the values of all features for the samples at these anomaly indices.
# 2. (Crucially) Investigating these timestamps in your LOG data and NETWORK data (once you implement collection for them).
# 3. Building the correlation logic (Phase 4) to see if metric anomalies align with suspicious log entries or network activity for specific pods.
# 4. Refining the anomaly detection models/thresholds based on your findings from investigation.